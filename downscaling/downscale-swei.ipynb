{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88d87a7-5cd2-4195-bcd5-9c69941241b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_119891/1809855210.py:20: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import fsspec\n",
    "import glob\n",
    "import intake\n",
    "import requests\n",
    "import datetime\n",
    "import gc  \n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from params import allnames\n",
    "\n",
    "from dask.diagnostics import progress\n",
    "from scipy.stats import norm\n",
    "from tqdm.autonotebook import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from params import allnames\n",
    "from params import homedir\n",
    "from params import experiment_ids, years, table_ids, labels, variables, savepath\n",
    "\n",
    "figdir = homedir + 'figures/'\n",
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from dask.diagnostics import progress\n",
    "from scipy.stats import norm\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from params import allnames, experiment_ids, variables, years, table_ids, labels\n",
    "from params import homedir, savepath\n",
    "\n",
    "## ## style ## ##\n",
    "xr.set_options(display_style='html')\n",
    "plt.style.use('./science.mplstyle')\n",
    "mpl.rcParams['axes.linewidth'] = 2\n",
    "mpl.rcParams['xtick.major.width'] = 2\n",
    "mpl.rcParams['ytick.major.width'] = 2\n",
    "mpl.rcParams['xtick.top']= False\n",
    "mpl.rcParams['ytick.right']= False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2330019e-e9df-4557-96fb-17409bb20ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## \n",
    "def wrfread(modeldir, gcm, exp, variant, domain, var):\n",
    "    all_files = sorted(os.listdir(modeldir))\n",
    "    read_files = []\n",
    "    for ii in all_files:\n",
    "        if (\n",
    "            ii.startswith(var + \".\")\n",
    "            #and gcm in ii\n",
    "            and variant in ii\n",
    "            and domain in ii\n",
    "            and exp in ii\n",
    "        ):\n",
    "            if domain in ii:\n",
    "                read_files.append(os.path.join(modeldir, str(ii)))\n",
    "    assert len(read_files) > 0, f\"No matching files found in {modeldir}\"\n",
    "\n",
    "    del all_files\n",
    "\n",
    "    data = xr.open_mfdataset(read_files, combine=\"by_coords\")\n",
    "    var_read = data.variables[var]\n",
    "\n",
    "    dates = []\n",
    "    for val in data[\"day\"].data:\n",
    "        try:\n",
    "            dates.append(datetime.datetime.strptime(str(val)[0:-2], \"%Y%m%d\").date())\n",
    "        except ValueError:\n",
    "            dates.append(datetime.datetime(int(str(val)[0:4]), int(str(val)[4:6]), 28))\n",
    "\n",
    "\n",
    "    var_read = xr.DataArray(var_read, dims=[\"day\", \"lat2d\", \"lon2d\"])\n",
    "    var_read[\"day\"] = dates\n",
    "    return var_read\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d1eac8f-d0d0-41c5-b69d-eacbc50d2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_swei(ds):\n",
    "    swe = ds['snow']\n",
    "    ntime = swe.shape[0]\n",
    "    nlat = swe.shape[2]\n",
    "    nlon = swe.shape[1]\n",
    "    nyr = int(ntime / 12)\n",
    "    nd = nlat * nlon\n",
    "    nm = 12\n",
    "\n",
    "    # Compute the 3-month cumulative sum for each pixel\n",
    "    ds_cumsum = ds.rolling(time=3, min_periods=3).sum()\n",
    "\n",
    "    years = np.unique(ds.time.dt.year)\n",
    "    months = np.unique(ds.time.dt.month)\n",
    "\n",
    "    # Reshape the data back into a 4D array of (year, month, lat, lon)\n",
    "    ds_new = xr.DataArray(\n",
    "        ds_cumsum['snow'].data.reshape((-1, 12, ds.sizes['lat'], ds.sizes['lon'])),\n",
    "        dims=('year', 'month', 'lat', 'lon'),\n",
    "        coords={'year': years, 'month': months, 'lat': ds['lat'], 'lon': ds['lon']}\n",
    "    )\n",
    "    categ = np.zeros((nyr, nm, nlon, nlat))\n",
    "    nsample = nyr\n",
    "    sweix = droughtindx(nsample)  # all values for each pixel.\n",
    "    sweix = np.array(sweix)\n",
    "\n",
    "    aindx = np.argsort(ds_new.data, axis=0)\n",
    "\n",
    "    # Create a broadcasting version of sweix\n",
    "    sweix_broadcasted = sweix[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "\n",
    "    # Assign sorted sweix values to categ based on sorted indices (array, indices, values, axis)\n",
    "    np.put_along_axis(categ, aindx,sweix_broadcasted, axis=0)\n",
    "\n",
    "    # Create the new xarray Dataset\n",
    "    ds_swei = xr.Dataset(\n",
    "        {'swei':(('year','month','lat','lon'), categ)},\n",
    "        coords={'year': years, 'month': months,'lat': ds['lat'], 'lon': ds['lon'], }\n",
    "    )\n",
    "    return ds_swei\n",
    "\n",
    "\n",
    "## use this\n",
    "def get_highest_month(ds,var = 'snow'):\n",
    "    tmp = ds[var].data.reshape(-1,12,ds[var].shape[1],ds[var].shape[2])\n",
    "    highest_month = np.argmax(tmp, axis=1)\n",
    "    coords = {'year': np.unique(ds.time.dt.year), 'lat' : ds.lat, 'lon' : ds.lon}\n",
    "    data = xr.DataArray(highest_month, dims=('year', 'lat','lon'), coords=coords)\n",
    "    return data\n",
    "\n",
    "def convert_year_month(ds,var):\n",
    "    years = np.unique(ds.time.dt.year)\n",
    "    months = np.unique(ds.time.dt.month)\n",
    "    \n",
    "    data = ds[var].data.reshape(-1,12,ds[var].shape[1],ds[var].shape[2])\n",
    "    # We then create a new dataset with year, month, lat, lon coordinates\n",
    "    ds_new = xr.Dataset(\n",
    "        {\n",
    "            var: ((\"year\", \"month\", \"lat\", \"lon\"), data),\n",
    "        },\n",
    "        coords={\n",
    "            \"year\": years,\n",
    "            \"month\": months,\n",
    "            \"lat\": ds.lat,\n",
    "            \"lon\": ds.lon,\n",
    "        },\n",
    "    )\n",
    "    ds_new.attrs = ds.attrs\n",
    "    return ds_new\n",
    "    \n",
    "## use this\n",
    "def collapse_to_highest_month(ds, var, snw_ds, convert = True):\n",
    "    highest_month = get_highest_month(snw_ds)\n",
    "    if convert:\n",
    "        ds = convert_year_month(ds, var)\n",
    "    highest_month = highest_month.drop('lat')\n",
    "    highest_month = highest_month.drop('lon')\n",
    "    result = ds.sel(month=highest_month, method='nearest').drop('month')\n",
    "    return result\n",
    "\n",
    "## usethis\n",
    "def get_3m_sum(data):\n",
    "    rolling_sum = data.rolling(time = 3, min_periods=3).sum()\n",
    "    return rolling_sum\n",
    "\n",
    "def get_sd_categ(swei, pr, tas, attrs):\n",
    "    swei_cond = swei < -0.8  # snow drought\n",
    "    pr_cond = pr < 0  # dry \n",
    "    tas_cond = tas > 0  # warm\n",
    "    \n",
    "    ds_new = xr.Dataset(\n",
    "        {\n",
    "            \"swei_cond\": ((\"year\", \"lat\", \"lon\"), swei_cond.swei.values),\n",
    "            \"pr_cond\": ((\"year\", \"lat\", \"lon\"), pr_cond.prec.values),\n",
    "            \"tas_cond\": ((\"year\", \"lat\", \"lon\"), tas_cond.t2.values),\n",
    "        },\n",
    "        coords={\n",
    "            \"year\": pr.year,\n",
    "            \"lat\": pr.lat,\n",
    "            \"lon\": pr.lon,\n",
    "        },\n",
    "    )\n",
    "    ds_new.attrs = attrs\n",
    "\n",
    "    return ds_new\n",
    "from scipy.stats import norm\n",
    "def droughtindx(nsample):\n",
    "    indx = []\n",
    "    for i in range(nsample):\n",
    "        px = (i+1-0.44)/(nsample+0.12)\n",
    "        indx.append(norm.ppf(px))\n",
    "    return indx\n",
    "\n",
    "\n",
    "def polarCentral_set_latlim(lat_lims, ax):\n",
    "    ax.set_extent([-180, 180, lat_lims[0], lat_lims[1]], ccrs.PlateCarree())\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    center, radius = [0.5, 0.5], 0.5\n",
    "    verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n",
    "    circle = mpath.Path(verts * radius + center)\n",
    "    ax.set_boundary(circle, transform=ax.transAxes)\n",
    "def reshape_3d(ds, var):\n",
    "    # Convert daily data to monthly data by taking the maximum value for each month\n",
    "    ds_sorted = ds.sortby('time')\n",
    "    ds_monthly = ds_sorted.resample(time='1M').max(dim='time')\n",
    "    # Extract year and month from the time coordinate\n",
    "    ds_monthly['year'] = ds_monthly['time.year']\n",
    "    ds_monthly['month'] = ds_monthly['time.month']\n",
    "    prepend_times = pd.date_range(start='1980-01-31', periods=8, freq='M')\n",
    "    append_times = pd.date_range(end='2100-09-30', periods=4, freq='M')\n",
    "    time_dataarray = ds_monthly.time\n",
    "    prepend_dataarray = xr.DataArray(prepend_times, dims=['time'])\n",
    "    append_dataarray = xr.DataArray(append_times, dims=['time'])\n",
    "    time_to_use = xr.concat([prepend_dataarray, time_dataarray, append_dataarray], dim='time')\n",
    "\n",
    "    month = np.unique(ds_monthly.month.data) #[9,10,11,12,1,  2,  3,  4,  5,  6,  7,  8]\n",
    "    year = np.unique(ds_monthly.year.data)\n",
    "    lat = ds_monthly.lat2d.data\n",
    "    lon = ds_monthly.lon2d.data\n",
    "    data =ds_monthly[var]\n",
    "    # Number of layers to add\n",
    "    n_front_layers = 8\n",
    "    n_end_layers = 4\n",
    "    pad_widths = [(n_front_layers, n_end_layers)] + [(0, 0)] * (data.ndim - 1)\n",
    "    arr_padded = np.pad(data, pad_widths, mode='constant', constant_values=np.nan)\n",
    "\n",
    "    #reshaped_data = arr_padded.reshape((-1,12,arr_padded.shape[1], arr_padded.shape[2]))\n",
    "    # Create xarray dataset\n",
    "    reshaped = xr.Dataset(\n",
    "        {\n",
    "            var: (['time', 'lat', 'lon'], arr_padded),\n",
    "        },\n",
    "        coords={\n",
    "            'time': time_to_use,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "        },\n",
    "    )\n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "576d5499-5049-4989-8e40-acfe5d449bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved/global/cfs/cdirs/m4099/cowherd/snow_c_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/t2_c_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/prec_c_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/snow_p_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/t2_p_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/prec_p_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/snow_r_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/t2_r_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/prec_r_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/snow_e_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/t2_e_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/prec_e_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/snow_l_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/t2_l_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/prec_l_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/snow_1_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/t2_1_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/prec_1_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/snow_5_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/t2_5_d01_bc.nc\n",
      "saved/global/cfs/cdirs/m4099/cowherd/prec_5_d01_bc.nc\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/global/cfs/cdirs/m4099/fate-of-snotel/wrfdata/access-cm2_r5i1p1f1_ssp370_bc/postprocess/d01/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssp370\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m datadir \u001b[38;5;241m=\u001b[39m basedir \u001b[38;5;241m+\u001b[39m mod_future \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/postprocess/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m domain \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 57\u001b[0m var_wrf_ssp370 \u001b[38;5;241m=\u001b[39m \u001b[43mwrfread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatadir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgcm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m var_wrf_ssp370 \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mscreen_times_wrf(var_wrf_ssp370, date_start_pd, date_end_pd)\n\u001b[1;32m     59\u001b[0m wrfdata \u001b[38;5;241m=\u001b[39m [var_wrf, var_wrf_ssp370]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mwrfread\u001b[0;34m(modeldir, gcm, exp, variant, domain, var)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrfread\u001b[39m(modeldir, gcm, exp, variant, domain, var):\n\u001b[0;32m----> 4\u001b[0m     all_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodeldir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m     read_files \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m all_files:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/global/cfs/cdirs/m4099/fate-of-snotel/wrfdata/access-cm2_r5i1p1f1_ssp370_bc/postprocess/d01/'"
     ]
    }
   ],
   "source": [
    "import util\n",
    "\n",
    "domains = ['d01']\n",
    "variables = ['snow', 't2', 'prec']\n",
    "gcms = ['cesm2','mpi-esm1-2-lr','cnrm-esm2-1',\n",
    "        'ec-earth3-veg','fgoals-g3','ukesm1-0-ll',\n",
    "        'canesm5','access-cm2','ec-earth3',]\n",
    "\n",
    "## TO DO : get data transferred for d01 for these: access, earth3 [no veg]\n",
    "## and run\n",
    "variants = ['r11i1p1f1','r7i1p1f1','r1i1p1f2',\n",
    "            'r1i1p1f1','r1i1p1f1','r2i1p1f2',\n",
    "            'r1i1p2f1','r5i1p1f1','r1i1p1f1',]\n",
    "      \n",
    "\n",
    "calendar = ['365_day','proleptic_gregorian','proleptic_gregorian',\n",
    "            'proleptic_gregorian','365_day','360_day',\n",
    "             '365_day','proleptic_gregorian', 'proleptic_gregorian',]\n",
    "\n",
    "ssps = ['ssp370','ssp370','ssp370','ssp370',\n",
    "        'ssp370','ssp370','ssp370','ssp370',\n",
    "        'ssp370']\n",
    "'''  \n",
    "\n",
    "domains = ['d03', 'd04']\n",
    "variables = ['snow', 't2', 'prec']\n",
    "gcms = ['ec-earth3-veg',]\n",
    "variants = ['r1i1p1f1',]\n",
    "calendar = ['proleptic_gregorian']\n",
    "ssps = ['ssp370']\n",
    "'''\n",
    "\n",
    "basedir = '/global/cfs/cdirs/m4099/fate-of-snotel/wrfdata/'\n",
    "model = None\n",
    "n = 0\n",
    "for domain in domains:\n",
    "    for idx, gcm in enumerate(gcms[n:]):\n",
    "        variant = variants[idx+n]\n",
    "        mod_historical = gcm +'_'+ variant + '_historical_bc'\n",
    "        mod_future = gcm +'_' + variant+ '_ssp370_bc'\n",
    "        datadir = basedir + mod_historical + '/postprocess/' + domain + '/'\n",
    "        for var in variables:\n",
    "            # historical\n",
    "            gcm = mod_historical\n",
    "            datadir = basedir + mod_historical + '/postprocess/' + domain + '/'\n",
    "            date_start_pd, date_end_pd = [1980, 1, 1], [2013, 12, 31]  # 30 years, historical\n",
    "            exp = \"hist\"\n",
    "            var_wrf = wrfread(datadir, gcm, exp, variant, domain, var)\n",
    "            var_wrf = util.screen_times_wrf(var_wrf, date_start_pd, date_end_pd)\n",
    "\n",
    "            # future \n",
    "            date_start_pd, date_end_pd = [2014, 1, 1], [2100, 12, 31]\n",
    "            gcm = mod_future\n",
    "            exp = \"ssp370\"\n",
    "            datadir = basedir + mod_future + '/postprocess/' + domain + '/'\n",
    "\n",
    "            var_wrf_ssp370 = wrfread(datadir, gcm, exp, variant, domain, var)\n",
    "            var_wrf_ssp370 = util.screen_times_wrf(var_wrf_ssp370, date_start_pd, date_end_pd)\n",
    "            wrfdata = [var_wrf, var_wrf_ssp370]\n",
    "            ds_concat = xr.concat(wrfdata, dim = 'day').to_dataset(name = var)\n",
    "            ds_concat = ds_concat.rename({'day':'time'})\n",
    "            ds_concat['time'] = ds_concat['time'].astype('datetime64')\n",
    "            try:\n",
    "                ds_concat.to_netcdf(f'{savepath}{var}_{gcms[idx+n]}_{domain}_bc.nc')\n",
    "                print('saved' + f'{savepath}{var}_{gcms[idx+n]}_{domain}_bc.nc')\n",
    "            except:\n",
    "                print('already loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "579108df-2ae2-49b1-add6-1a7a6728d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved /global/cfs/cdirs/m4099/cowherd/categs_snow_cesm2_d01_bc.nc\n",
      "saved /global/cfs/cdirs/m4099/cowherd/categs_snow_mpi-esm1-2-lr_d01_bc.nc\n",
      "saved /global/cfs/cdirs/m4099/cowherd/categs_snow_cnrm-esm2-1_d01_bc.nc\n",
      "saved /global/cfs/cdirs/m4099/cowherd/categs_snow_ec-earth3-veg_d01_bc.nc\n",
      "saved /global/cfs/cdirs/m4099/cowherd/categs_snow_fgoals-g3_d01_bc.nc\n",
      "saved /global/cfs/cdirs/m4099/cowherd/categs_snow_ukesm1-0-ll_d01_bc.nc\n",
      "saved /global/cfs/cdirs/m4099/cowherd/categs_snow_canesm5_d01_bc.nc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "domains = ['d01']\n",
    "variables = ['snow', 't2', 'prec']\n",
    "gcms = ['cesm2','mpi-esm1-2-lr','cnrm-esm2-1',\n",
    "        'ec-earth3-veg','fgoals-g3','ukesm1-0-ll',\n",
    "        'canesm5']#,'access-cm2','ec-earth3',]\n",
    "\n",
    "\n",
    "variants = ['r11i1p1f1','r7i1p1f1','r1i1p1f2',\n",
    "            'r1i1p1f1','r1i1p1f1','r2i1p1f2',\n",
    "            'r1i1p2f1','r5i1p1f1','r1i1p1f1',]\n",
    "      \n",
    "\n",
    "calendar = ['365_day','proleptic_gregorian','proleptic_gregorian',\n",
    "            'proleptic_gregorian','365_day','360_day',\n",
    "             '365_day','proleptic_gregorian', 'proleptic_gregorian',]\n",
    "\n",
    "ssps = ['ssp370','ssp370','ssp370','ssp370',\n",
    "        'ssp370','ssp370','ssp370','ssp370',\n",
    "        'ssp370']\n",
    "\n",
    "'''\n",
    "domains = ['d03', 'd04']\n",
    "variables = ['snow', 't2', 'prec']\n",
    "gcms = ['ec-earth3-veg',]\n",
    "variants = ['r1i1p1f1',]\n",
    "calendar = ['proleptic_gregorian']\n",
    "ssps = ['ssp370']\n",
    "'''\n",
    "gc.collect()\n",
    "datasets = {}\n",
    "\n",
    "for domain in domains:\n",
    "    for gcm in gcms:\n",
    "        filenames = [f\"{savepath}{var}_{gcm}_{domain}_bc.nc\" for var in variables]\n",
    "        for filename in filenames:\n",
    "            datasets[filename.split('/')[-1].split('.')[0]] = xr.open_dataset(filename)\n",
    "            \n",
    "datasets_3m_sum = {}\n",
    "swei_datasets = {}\n",
    "for name, ds in datasets.items():\n",
    "    var = name.split('_')[0]\n",
    "    ds = reshape_3d(ds,var)\n",
    "    datasets_3m_sum[name] = get_3m_sum(ds)\n",
    "    if var == 'snow':\n",
    "        swei_datasets[name] = get_swei(ds)\n",
    "\n",
    "datasets_3m_sum_maxsnw = {}\n",
    "for name, ds in datasets_3m_sum.items():\n",
    "    var = name.split('_')[0]\n",
    "    snw_ds = datasets_3m_sum[f'snow_{name.split(f\"{var}_\")[-1]}']\n",
    "    datasets_3m_sum_maxsnw[name] = collapse_to_highest_month(ds, var, snw_ds, True)\n",
    "\n",
    "\n",
    "swei_datasets_maxsnw = {}\n",
    "for name, ds in swei_datasets.items():\n",
    "    var = name.split('_')[0]\n",
    "    snw_long = datasets_3m_sum[f'snow_{name.split(f\"{var}_\")[-1]}']\n",
    "    tmp = collapse_to_highest_month(ds, 'swei', snw_long, False)\n",
    "    tmp.to_netcdf(f'{savepath}swei_max_{name}.nc')\n",
    "    swei_datasets_maxsnw[f'{name}'] = tmp\n",
    "categs = {}\n",
    "for name, swei in swei_datasets_maxsnw.items():\n",
    "    var = name.split('_')[0]\n",
    "    \n",
    "    pr_long = datasets_3m_sum_maxsnw[f'prec_{name.split(f\"{var}_\")[-1]}']\n",
    "    pr_anom = pr_long - pr_long.sel(year=slice(pr_long.year[0], pr_long.year[49])).mean(dim='year') ## val minus average\n",
    "\n",
    "    tas_long = datasets_3m_sum_maxsnw[f't2_{name.split(f\"{var}_\")[-1]}']\n",
    "    tas_anom = tas_long - tas_long.sel(year=slice(tas_long.year[0], tas_long.year[49])).mean(dim='year') ## val minus average\n",
    "\n",
    "    tmp = get_sd_categ(swei, pr_anom ,tas_anom, pr_long.attrs)\n",
    "    try:\n",
    "        tmp.to_netcdf(f'{savepath}categs_{name}.nc')\n",
    "        print('saved ' + f'{savepath}categs_{name}.nc')\n",
    "    except:\n",
    "        continue\n",
    "    categs[name] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6427546f-3fad-4801-b454-0be41712e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(ds, var):\n",
    "    # Convert daily data to monthly data by taking the maximum value for each month\n",
    "    ds_sorted = ds.sortby('time')\n",
    "    ds_monthly = ds_sorted.resample(time='1M').max(dim='time')\n",
    "    # Extract year and month from the time coordinate\n",
    "    ds_monthly['year'] = ds_monthly['time.year']\n",
    "    ds_monthly['month'] = ds_monthly['time.month']\n",
    "    ds_monthly = ds_monthly.drop('time')\n",
    "    month = np.unique(ds_monthly.month.data) #[9,10,11,12,1,  2,  3,  4,  5,  6,  7,  8]\n",
    "    year = np.unique(ds_monthly.year.data)\n",
    "    lat = ds_monthly.lat2d.data\n",
    "    lon = ds_monthly.lon2d.data\n",
    "    data =ds_monthly[var]\n",
    "    # Number of layers to add\n",
    "    n_front_layers = 8\n",
    "    n_end_layers = 4\n",
    "    pad_widths = [(n_front_layers, n_end_layers)] + [(0, 0)] * (data.ndim - 1)\n",
    "    arr_padded = np.pad(data, pad_widths, mode='constant', constant_values=np.nan)\n",
    "\n",
    "    reshaped_data = arr_padded.reshape((-1,12,arr_padded.shape[1], arr_padded.shape[2]))\n",
    "    # Create xarray dataset\n",
    "    reshaped = xr.Dataset(\n",
    "        {\n",
    "            var: (['year','month', 'lat', 'lon'], reshaped_data),\n",
    "        },\n",
    "        coords={\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "        },\n",
    "    )\n",
    "    return reshaped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e3smeval",
   "language": "python",
   "name": "e3smeval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
